{
    "humaneval": {
        "pass@1": 0.05426829268292683,
        "pass@10": 0.09146341463414634
    },
    "config": {
        "prefix": "",
        "do_sample": true,
        "temperature": 0.2,
        "top_k": 0,
        "top_p": 0.95,
        "n_samples": 10,
        "eos": "<|endoftext|>",
        "seed": 0,
        "model": "HuggingFaceTB/SmolLM2-135M",
        "modeltype": "causal",
        "peft_model": null,
        "revision": null,
        "use_auth_token": false,
        "trust_remote_code": false,
        "tasks": "humaneval",
        "instruction_tokens": null,
        "batch_size": 1024,
        "max_length_generation": 500,
        "precision": "bf16",
        "load_in_8bit": false,
        "load_in_4bit": false,
        "left_padding": false,
        "limit": null,
        "limit_start": 0,
        "save_every_k_tasks": -1,
        "postprocess": true,
        "allow_code_execution": true,
        "generation_only": false,
        "load_generations_path": null,
        "load_data_path": null,
        "metric_output_path": "evaluation_results.json",
        "save_generations": false,
        "load_generations_intermediate_paths": null,
        "save_generations_path": "generations.json",
        "save_references": false,
        "save_references_path": "references.json",
        "prompt": "prompt",
        "max_memory_per_gpu": null,
        "check_references": false
    }
}